{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R_mOBdRxcWyB"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import string\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxT-Sh4CVCbZ",
        "outputId": "31914eb4-b8f0-40e1-9dcc-d4f05e6b066d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\alkal\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\alkal\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "boCkJm41cp8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'none'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# death_row_fn = 'gdrive/My Drive/COEN140/group-project/data/Last-Statement-of-Death-Row.csv'\n",
        "# suicide_depression_fn = 'gdrive/My Drive/COEN140/group-project/data/reddit_depression_suicidewatch.csv'\n",
        "# hate_fn = 'gdrive/My Drive/COEN140/group-project/data/Dynamically_Generated_Hate_Dataset_v0.2.3.csv'\n",
        "# sentiment_fn = 'gdrive/My Drive/COEN140/group-project/data/sentiment.csv'\n",
        "# output_fn = 'gdrive/My Drive/COEN140/group-project/data/output.dat'\n",
        "\n",
        "#read train.csv file into dataframe\n",
        "#data was pre-processed, cleaned in 'data_collection.ipynb'\n",
        "data = pd.read_csv('train.csv', header=0)\n",
        "df = data.iloc[:,1:]\n",
        "df.dropna() #drop empty rows from dataframe\n",
        "#fixes bug: thinks row 82162 is nan which messes up the pre-processing\n",
        "df.iloc[82162,0] = \"none\"\n",
        "df.iloc[82162,0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "izyMJoa61qR6"
      },
      "source": [
        "Class labels are \n",
        "\n",
        "0 - Neutral \n",
        "\n",
        "1 - Hate\n",
        "\n",
        "2 - Depression\n",
        "\n",
        "3 - Suicidal"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TCEdPNwIU5fa"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zjsvL2N8UnNL"
      },
      "outputs": [],
      "source": [
        "# split feature matrix and target values\n",
        "docs = df['text']\n",
        "classes = df['class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L5XWwOJTNHCQ"
      },
      "outputs": [],
      "source": [
        "# split each text sample into seperate words \n",
        "docs_mat = [word_tokenize(text) for text in docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fjssRgvhU4X3"
      },
      "outputs": [],
      "source": [
        "sw_nltk = stopwords.words('english')\n",
        "s = PorterStemmer()\n",
        "def preprocess_docs(docs): \n",
        "  ''' Taking a matrix of documents with words in each document, \n",
        "  preprocess the matrix by removing punctuation, words less than 4 letters, \n",
        "  and stop words and return the preprocessed matrix. '''\n",
        "\n",
        "  docs = [ [ s.stem(word) for word in sample if (word not in string.punctuation) and len(word) >= 4 and (word.lower() not in sw_nltk)] for sample in docs]\n",
        "  return docs \n",
        "\n",
        "pp_docs = preprocess_docs(docs_mat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fxIkHRaMrhwt"
      },
      "outputs": [],
      "source": [
        "def truncate(docs, classes): \n",
        "  # standardize each sample to have the same number of words \n",
        "  trunc = int(np.mean([len(d) for d in docs]))\n",
        "\n",
        "  return [ (' '.join(docs[i][:trunc]), classes[i]) for i in range(len(docs)) if len(docs[i]) >= trunc]\n",
        "\n",
        "\n",
        "trunc = truncate(pp_docs, classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aFCorDXx30Pq"
      },
      "outputs": [],
      "source": [
        "trunc_docs = [t[0] for t in trunc]\n",
        "trunc_classes = [t[1] for t in trunc]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "y1sANCmKS68D"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(list,\n",
              " str,\n",
              " 'distil gasif wast inciner method accord present invent distil gasif wast inciner method plural distil furnac provid combust furnac wast held distil furnac dry-distil sequenc therebi produc combust control carri temperatur combust furnac becom predetermin first temperatur case combust introduc combust furnac burnt method includ step suppli oxygen requir distil wast first distil furnac control degre open first valv provid first oxygen suppli passag temperatur combust furnac becom first temperatur combust combust case combust produc dry-distil wast held first distil furnac use oxygen suppli first distil furnac first oxygen suppli passag oxygen suppli sourc combust introduc combust furnac burnt step detect presenc wast second distil furnac control carri temperatur combust furnac becom first temperatur combust combust produc first distil furnac ignit wast held second distil furnac use oxygen suppli second distil furnac second oxygen suppli passag oxygen suppli sourc step dry-distil wast held second distil furnac use oxygen suppli second distil furnac second oxygen suppli passag oxygen suppli sourc produc combust introduc combust produc second distil furnac combust furnac start combust distil gasif wast inciner method accord present invent first combust produc first distil furnac dry-distil wast held furnac use oxygen suppli')"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_train, docs_test, cls_train, cls_test = train_test_split(trunc_docs, trunc_classes, train_size=0.7, test_size=0.3 , shuffle=True, random_state=1)\n",
        "type(docs_train), type(docs_train[0]), docs_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BugnJ-JRfRaQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(list,\n",
              " list,\n",
              " ['distil',\n",
              "  'gasif',\n",
              "  'wast',\n",
              "  'inciner',\n",
              "  'method',\n",
              "  'accord',\n",
              "  'present',\n",
              "  'invent',\n",
              "  'distil',\n",
              "  'gasif',\n",
              "  'wast',\n",
              "  'inciner',\n",
              "  'method',\n",
              "  'plural',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'provid',\n",
              "  'combust',\n",
              "  'furnac',\n",
              "  'wast',\n",
              "  'held',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'dry-distil',\n",
              "  'sequenc',\n",
              "  'therebi',\n",
              "  'produc',\n",
              "  'combust',\n",
              "  'control',\n",
              "  'carri',\n",
              "  'temperatur',\n",
              "  'combust',\n",
              "  'furnac',\n",
              "  'becom',\n",
              "  'predetermin',\n",
              "  'first',\n",
              "  'temperatur',\n",
              "  'case',\n",
              "  'combust',\n",
              "  'introduc',\n",
              "  'combust',\n",
              "  'furnac',\n",
              "  'burnt',\n",
              "  'method',\n",
              "  'includ',\n",
              "  'step',\n",
              "  'suppli',\n",
              "  'oxygen',\n",
              "  'requir',\n",
              "  'distil',\n",
              "  'wast',\n",
              "  'first',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'control',\n",
              "  'degre',\n",
              "  'open',\n",
              "  'first',\n",
              "  'valv',\n",
              "  'provid',\n",
              "  'first',\n",
              "  'oxygen',\n",
              "  'suppli',\n",
              "  'passag',\n",
              "  'temperatur',\n",
              "  'combust',\n",
              "  'furnac',\n",
              "  'becom',\n",
              "  'first',\n",
              "  'temperatur',\n",
              "  'combust',\n",
              "  'combust',\n",
              "  'case',\n",
              "  'combust',\n",
              "  'produc',\n",
              "  'dry-distil',\n",
              "  'wast',\n",
              "  'held',\n",
              "  'first',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'use',\n",
              "  'oxygen',\n",
              "  'suppli',\n",
              "  'first',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'first',\n",
              "  'oxygen',\n",
              "  'suppli',\n",
              "  'passag',\n",
              "  'oxygen',\n",
              "  'suppli',\n",
              "  'sourc',\n",
              "  'combust',\n",
              "  'introduc',\n",
              "  'combust',\n",
              "  'furnac',\n",
              "  'burnt',\n",
              "  'step',\n",
              "  'detect',\n",
              "  'presenc',\n",
              "  'wast',\n",
              "  'second',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'control',\n",
              "  'carri',\n",
              "  'temperatur',\n",
              "  'combust',\n",
              "  'furnac',\n",
              "  'becom',\n",
              "  'first',\n",
              "  'temperatur',\n",
              "  'combust',\n",
              "  'combust',\n",
              "  'produc',\n",
              "  'first',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'ignit',\n",
              "  'wast',\n",
              "  'held',\n",
              "  'second',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'use',\n",
              "  'oxygen',\n",
              "  'suppli',\n",
              "  'second',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'second',\n",
              "  'oxygen',\n",
              "  'suppli',\n",
              "  'passag',\n",
              "  'oxygen',\n",
              "  'suppli',\n",
              "  'sourc',\n",
              "  'step',\n",
              "  'dry-distil',\n",
              "  'wast',\n",
              "  'held',\n",
              "  'second',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'use',\n",
              "  'oxygen',\n",
              "  'suppli',\n",
              "  'second',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'second',\n",
              "  'oxygen',\n",
              "  'suppli',\n",
              "  'passag',\n",
              "  'oxygen',\n",
              "  'suppli',\n",
              "  'sourc',\n",
              "  'produc',\n",
              "  'combust',\n",
              "  'introduc',\n",
              "  'combust',\n",
              "  'produc',\n",
              "  'second',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'combust',\n",
              "  'furnac',\n",
              "  'start',\n",
              "  'combust',\n",
              "  'distil',\n",
              "  'gasif',\n",
              "  'wast',\n",
              "  'inciner',\n",
              "  'method',\n",
              "  'accord',\n",
              "  'present',\n",
              "  'invent',\n",
              "  'first',\n",
              "  'combust',\n",
              "  'produc',\n",
              "  'first',\n",
              "  'distil',\n",
              "  'furnac',\n",
              "  'dry-distil',\n",
              "  'wast',\n",
              "  'held',\n",
              "  'furnac',\n",
              "  'use',\n",
              "  'oxygen',\n",
              "  'suppli'])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#split all of the words in the samples within the train and test documents\n",
        "docs_train2 = [doc.split() for doc in docs_train]\n",
        "docs_test2 = [doc.split() for doc in docs_test]\n",
        "type(docs_train2), type(docs_train2[0]), docs_train2[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tJZ-6HKRQckW"
      },
      "source": [
        "# Model Selection "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kiIpR_vcUKnv"
      },
      "outputs": [],
      "source": [
        "def score_models(models, docs_train, cls_train): \n",
        "  scores = []\n",
        "  for m in models: \n",
        "\n",
        "    # take the average accuracy score for the model across k-fold cross validation\n",
        "    scores.append((m, np.mean(cross_val_score(m, X=docs_train, y=cls_train, scoring='accuracy'))))\n",
        "  \n",
        "  # sort the scores by the model with the highest accuracy \n",
        "  scores.sort(key=lambda x: x[1], reverse=True)\n",
        "  return scores "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zw1UJC8i3RUg"
      },
      "source": [
        "Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMdUeMbf3RGK",
        "outputId": "b344ddc1-c367-4633-d1c3-edc562154d75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(SGDClassifier(alpha=0.001, max_iter=5, random_state=0, tol=None),\n",
              "  0.9869172758263914)]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipe = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer())])\n",
        "\n",
        "#the model was not functioning properly when passing in docs_train after splitting all of the words in the samples\n",
        "score_models([SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=0, max_iter=5, tol=None)], pipe.fit_transform(docs_train), cls_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssM7bOa_3pvl"
      },
      "outputs": [],
      "source": [
        "# def get_best_svm(): \n",
        "  \n",
        "#   # linear SVM with stochastic gradient descent \n",
        "#   svm_models = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=0, max_iter=5, tol=None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b-hyP40q24Gs"
      },
      "source": [
        "Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "rdsaYgbw26h7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(MultinomialNB(), 0.9910185964517945)]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "pipe = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer())])\n",
        "\n",
        "#the model was not functioning properly when passing in docs_train after splitting all of the words in the samples\n",
        "score_models([MultinomialNB()], pipe.fit_transform(docs_train), cls_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8XgP58ClR4-b"
      },
      "source": [
        "Classic Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "bfcR4a-aT1Zg"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 889, in fit\n    super().fit(\n  File \"c:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n    self._validate_params()\n  File \"c:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n    validate_parameter_constraints(\n  File \"c:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'min_samples_leaf' parameter of DecisionTreeClassifier must be an int in the range [1, inf) or a float in the range (0.0, 1.0). Got None instead.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[38], line 12\u001b[0m\n\u001b[0;32m      5\u001b[0m   dtc_models \u001b[39m=\u001b[39m [  DecisionTreeClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, max_depth\u001b[39m=\u001b[39md, min_samples_leaf\u001b[39m=\u001b[39ml, min_impurity_decrease\u001b[39m=\u001b[39mi) \n\u001b[0;32m      6\u001b[0m   \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m impurity_decrease_range  \n\u001b[0;32m      7\u001b[0m   \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m leaf_samples_range \n\u001b[0;32m      8\u001b[0m   \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m depth_range ]\n\u001b[0;32m     10\u001b[0m   \u001b[39mreturn\u001b[39;00m score_models(dtc_models, docs_train, cls_train)\n\u001b[1;32m---> 12\u001b[0m scores \u001b[39m=\u001b[39m get_best_dtc()\n\u001b[0;32m     13\u001b[0m scores[\u001b[39m0\u001b[39m]\n",
            "Cell \u001b[1;32mIn[38], line 10\u001b[0m, in \u001b[0;36mget_best_dtc\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m impurity_decrease_range \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39m5\u001b[39m)\n\u001b[0;32m      5\u001b[0m dtc_models \u001b[39m=\u001b[39m [  DecisionTreeClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, max_depth\u001b[39m=\u001b[39md, min_samples_leaf\u001b[39m=\u001b[39ml, min_impurity_decrease\u001b[39m=\u001b[39mi) \n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m impurity_decrease_range  \n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m leaf_samples_range \n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m depth_range ]\n\u001b[1;32m---> 10\u001b[0m \u001b[39mreturn\u001b[39;00m score_models(dtc_models, docs_train, cls_train)\n",
            "Cell \u001b[1;32mIn[21], line 6\u001b[0m, in \u001b[0;36mscore_models\u001b[1;34m(models, docs_train, cls_train)\u001b[0m\n\u001b[0;32m      2\u001b[0m scores \u001b[39m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m models: \n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m   \u001b[39m# take the average accuracy score for the model across k-fold cross validation\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m   scores\u001b[39m.\u001b[39mappend((m, np\u001b[39m.\u001b[39mmean(cross_val_score(m, X\u001b[39m=\u001b[39;49mdocs_train, y\u001b[39m=\u001b[39;49mcls_train, scoring\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m))))\n\u001b[0;32m      8\u001b[0m \u001b[39m# sort the scores by the model with the highest accuracy \u001b[39;00m\n\u001b[0;32m      9\u001b[0m scores\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m    266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;00m train, test \u001b[39min\u001b[39;00m cv\u001b[39m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[1;32m--> 285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(scoring):\n",
            "File \u001b[1;32mc:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
            "\u001b[1;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 889, in fit\n    super().fit(\n  File \"c:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 177, in fit\n    self._validate_params()\n  File \"c:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n    validate_parameter_constraints(\n  File \"c:\\Users\\alkal\\anaconda3\\envs\\c140\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'min_samples_leaf' parameter of DecisionTreeClassifier must be an int in the range [1, inf) or a float in the range (0.0, 1.0). Got None instead.\n"
          ]
        }
      ],
      "source": [
        "def get_best_dtc(): \n",
        "  depth_range = (None, 1, 10, 20, 50)\n",
        "  leaf_samples_range = (None, 1, 10, 20, 50)\n",
        "  impurity_decrease_range = range(0,5)\n",
        "  dtc_models = [  DecisionTreeClassifier(random_state=0, max_depth=d, min_samples_leaf=l, min_impurity_decrease=i) \n",
        "  for i in impurity_decrease_range  \n",
        "  for l in leaf_samples_range \n",
        "  for d in depth_range ]\n",
        "\n",
        "  return score_models(dtc_models, docs_train, cls_train)\n",
        "\n",
        "scores = get_best_dtc()\n",
        "scores[0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
